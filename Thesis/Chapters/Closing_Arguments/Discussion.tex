\section{Discussion}

%Ts: 
%Pc: What can be observed by empirical data
% From the above results, there are mainly two things to note for rating the performance from our commercial eye tracker. First, it appears that the tracker is plenty capable of providing both accurate and abundant data for this particular use case. However, we might have seen even better results had the sampling rate been higher and illumination better. From table \_, we can observe a data range of \_ with as little as \_ noise. As is also made clear by figure \ref{fig:res_FirstHeatMapTest}, this suggests that the eye tracker is reliably able to distinguish between a wide range of individual gaze points. Finally, there is a clear correlation between sample labels and the feature set, demonstrated by figure \_. While this is also very dependent on the manual choice of features, it is still promising for the statistical inference that the data is coherent and representative of its labels.

% From the above results, there are mainly two things to 

\subsection{Data Quality}

From the data quality results of section \ref{sec:res_DataQuality}, we get a broad overview of the performance of Tobii ET5. The data quality is good, abundant, and relatively frequent. However, as is immediately apparent from both figures \ref{fig:res_GazePointTest} and \ref{fig:res_ScanpathTest}, there is a significant effect of bias, and some points on-screen are more disturbed than others. Corner points, particularly top corner points, seem to consistently output values that are slightly off their target values. This result is reasonable, considering how it calculates on-screen gaze points and where the tracker is mounted. By the horizontal fashion in which the IR- and camera sensors are placed in the tracker's casing, it is not unnatural that noise and measurement error is more pronounced for the detection of vertical eye movements. Additionally, since the tracker's mounting point is right below the center of the monitor, measurement error becomes more and more apparent the farther one is looking from this point
%The same effect of diminishing data quality at distant points from the sensor can also be seen on variance. 

Scanpath test Discussion
In terms of data consistency, the results from \ref{fig:res_ScanpathTest} are very interesting. From this plot, we can see that although the scan paths tracked by Tobii ET5 are slightly off their target, they are remarkably consistent between consecutive tests. This goes to show that the bias observed in both the gaze point and scan path tests might primarily be caused by poor tracker calibration and/or difficulties in designing a system that can discern gaze points universally between a wide variety of monitors and recording environments. Another interesting result from the scanpath test is the peculiar dip in y-position when passing along the top border of the monitor. One explaination for this effect might be that the internal processing within the tracker recognizes that it consistently outputs values that are outide of the defined [0.0, 1.0] range, and attempts to correct accordingly. Another, and perhaps more likely explaination is that the sensor has trouble discerning purely lateral movements and slightly tilted lateral movements when gaze is on the very ends of the screen, where it already struggles the most with gaze calculation. Conversely, the scanpath is very accurate on the opposite end of the screen, right above where the tracker is mounted.
% Impossible to keep environment completely agnostic for each test. Likely causes flaws in consistency.

The variance in data output is visualized in figure \ref{fig:res_DataDeviations} and given with hard numbers in table \ref{tab:res_DataStats}. However, since these results are sadly based on just one empirical test and we have established that the Tobii ET5 output rather unstable data, they should be taken with a grain of salt. From the violins of each plot, we can see deviations in coordinates from the mean value of one point on one axis. What is especially interesting here reflects what was discussed above, which is the fading data quality at top and edge target points compared to the center point. The effect is also particularly noticable on the y-axis, with a generally higher deviations on the top row of violin plots, as well as the absolute value of the second std-entry in the coordinate tuples of \ref{tab:res_DataStats}. The exact same consequence is seen in the bias tuples, with higher values on the y-axis entry seven out of nine times. The bias entries also and almost consistently tend upwards, indicated by negative values on the y-axis. This evidence, along with the fac that all variance entries lie within relatively low magnitudes, argue for the point made above. We can therefore confidently conclude that the data output from Tobii ET5, although fairly biased towards higher vertical points, is very accurate.
%excluding possibly some edge points, lie confidently within the magnitude of about $5*10^(-3)$. 

Another thing to note is the shape of the violins of figure \ref{fig:res_DataDeviations}, with either long tails in one direction, or something resembling two gaussian distributions with a "dip" in between. If the data were only affected by bias and variance, one would expect to see only one gaussian distribution, centered on the mean value of all samples (or the target point if there were no bias). The results we see here, however, are likely caused by data drift, and what shapes the drift produces depends on whether the signal continuously drifts in one direction during the course of recording, or if it at some point "turns" and drifts in the opposite direction. In fact, if we take a closer look at the data gathered by the three-second fixation tests at the top left point of figure \ref{fig:res_GazePointTest}, we can observe the very erratic gaze behaviour of figure \ref{fig:res_DataDrift}. From these results, it is clear that drift is significant. In fact, it seems that there is little noise in the data at all, and that the data deviations discussed above might be entirely caused by data drift. As detailed in section \ref{sec:hwds_TobiiEyeTracker5}, however, the "raw" data we get from Tobii ET5 is in reality lightly filtered for stability. Given that we have no control over the internal processing done by Tobii, it is therefore difficult to determine whether data drift is explainable and/or avoidable. Even so, some of it might be explained by ocular or physical noise that likely causes trouble for the Tobii internals. Physical noise refers to that of slight head or body movements, and a typical type of ocular noise are for instance microsaccades, detailed in section \ref{sec:bt_TheOculomotorSystem}. 
%Since this processing must be very sensitive, 

Finally, we see an actual use case for the eye tracker in figure \ref{fig:res_PaperHeatmap}. From this plot, none of the shortcomings discussed above is truly noticable except perhaps some effefts of rightward bias in the lower regions of the screen, causing distortion. Nonetheless, one can clearly distinguish fixations on paragraphs from one another, and in some cases even individual lines of text. The value of this data is self-evident. From this simple test, one can for instance infer that the subject has paid especial attention to the abstract, while seemingly glancing over other parts of the text. He has also completely ignored the header and foot-notes, and of the authors, he was the most focused on capturing, or remembering, the main author.

%as the on-screen gaze-point is calculated from the angular position of the pupil, as well as point of origin and distance from screen.

\subsection{Labeled data set}

Something about the labeling process and the dataset processed for furhter classification

\subsection{Classification}

Looking at section \_, it can further be observed that even with a limited supply of decently labeled data sets, we can train multiple models that surpass the classification accuracies of \_ algorithm in the binary classification problem. Even concerning multi-class classification, some of our models perform well given the underlying data. 


%Ts: 
%Pc: Why is this observed


%Ts: 
%Pc: What would have been observed if methods were different

%Ts: 
%Pc: How could methods be different and why wasn't this done in the first place?

\subsection{Model Improvements}
While the models presented do solve our initial problem of eye event classification, they are likely not even close to the most optimal given the classification problem. Sadly, I did not research the prospects enough to implement such a model. Some solutions come to mind, however. For instance, one particular shortcoming of using a classical machine learning model is the need for manually designed features. As described in section \ref{sec:meth_FeatureGeneration}, all features used for classification are selected by an educated assumption of its inherent correlation with eye movement. However, an educated assumption is still merely an assumption, so 
the model would likely significantly improve if it generated its features as a set of hidden layers in a neural network.


% Remember to be critical of own methods.