\chapter*{Abstract} 
\addcontentsline{toc}{chapter}{Abstract}

Esports is a growing industry, and as kids and adults get progressively into the field of competitive gaming, the need for concrete feedback for performance advancements arises. However, most forms of feedback come from direct coaching, which is no good alternative for the casual gamer. In the day and age of big data and machine learning, we believe that both simple and complex analysis can be inferred at a low cost and with great availability. 
Using eye movement data gathered from interactive gaming environments, this project seeks to classify segments of a time series according to the oculomotor events of the eye. These segments can further be utilized as features in more complex machine learning models for the purposes of performance analytics. 

Traditionally, methods for eye tracking data segmentation have relied on manually coded algorithms. This introduces challenges for the algorithm implementer, since every use case requires manually defined parameters, which usually varies greatly between implementations. Because of this, I chose to design a machine learning model using gradient boost that in essence sets these parameters automatically by learning patterns from a large, labelled dataset. The need for labelled datasets was solved by designing a digital environment where eye tracking data from multiple users could be labelled effectively. 

I found that data labelling is a very time consuming and complex process, and the labelling quality greatly influences the machine learning model performance. The interactive environment introduces uncertainty which further degrades data quality. However, even with poorly labelled datasets, the model achieved a significant degree of accuracy, which means that there is great potential in the method. Future research should apply serious effort into effective data labelling, and perhaps investigate model accuracy improvements by pre training on publicly available eye tracking datasets, manually labelled by eye research professionals.
